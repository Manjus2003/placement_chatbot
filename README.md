# ğŸ“ Placement Companion Chatbot

An intelligent **Agentic RAG-based conversational AI system** designed to help M.Tech students at MSIS, MAHE access placement-related information through natural language queries.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)
![Pinecone](https://img.shields.io/badge/Pinecone-Vector%20DB-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸŒŸ Key Features

- **Dual RAG Modes**
  - **Basic RAG**: Fast semantic search for simple queries
  - **Agentic RAG**: Multi-step reasoning for complex questions

- **11 Specialized Tools**
  - Vector Search, Company Comparison, Eligibility Check
  - Trend Analysis, Skill Demand, Personalized Recommendations
  - And 5 more analytics tools

- **16 Query Type Classifications**
  - From simple company info to complex statistical analysis

- **Quality Assurance System**
  - Built-in Critic component that evaluates answers before delivery
  - Iterative refinement (up to 3 cycles) for complex queries

- **Conversational Memory**
  - Remembers context across chat turns
  - Resolves pronouns and references intelligently

## ğŸ—ï¸ Architecture

```
User Query â†’ Input Validation â†’ Query Routing â†’ Processing â†’ Response
                                      â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Basic RAG            â”‚
                        â”‚    or                   â”‚
                        â”‚    Agentic RAG          â”‚
                        â”‚    (Plan-Execute-Eval)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agentic RAG Components

1. **Planner**: Analyzes query and creates execution plan
2. **Executor**: Runs specialized tools step-by-step
3. **Critic**: Evaluates quality and decides (Accept/Refine/Replan)

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Streamlit |
| **Vector Database** | Pinecone (Serverless) |
| **Embedding Model** | Alibaba GTE-Qwen2-7B-Instruct (384D) |
| **LLM** | Qwen2.5-72B via HuggingFace Router |
| **Reranker** | Cross-Encoder MS-MARCO-MiniLM-L6-v2 |
| **OCR** | DeepSeek OCR |
| **Classification** | DeBERTa-v3-large (Zero-shot) |
| **Language** | Python 3.10+ |

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10 or higher
- pip package manager
- API keys (Pinecone, HuggingFace)

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/Manjus2003/placement_chatbot.git
   cd placement_chatbot
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   
   Create a file named `environment.env` in the root directory:
   ```env
   PINECONE_API_KEY=your_pinecone_api_key
   HF_TOKEN=your_huggingface_token
   PINECONE_INDEX_NAME=placement-companion-v5
   PINECONE_CLOUD=aws
   PINECONE_REGION=us-east-1
   ```

4. **Run the application**
   ```bash
   streamlit run streamlit_ui_v2.py
   ```

5. **Access the UI**
   
   Open your browser and navigate to: `http://localhost:8501`

## ğŸš€ Usage

### Basic Queries
```
User: "What is Amazon's CTC?"
Bot: [Returns salary information with sources]
```

### Complex Queries
```
User: "Compare Amazon and Google salaries and tell me if I'm eligible with 8.5 CGPA in CSE"
Bot: [Executes multi-step plan]
     1. Searches Amazon data
     2. Searches Google data
     3. Compares packages
     4. Checks eligibility
     5. Generates comparison table
```

### Follow-up Questions
```
User: "What about their interview process?"
Bot: [Remembers context and provides interview info for Amazon & Google]
```

## ğŸ“‚ Project Structure

```
placement_chatbot/
â”œâ”€â”€ agentic/                          # Agentic RAG module
â”‚   â”œâ”€â”€ agentic_rag.py               # Main orchestrator
â”‚   â”œâ”€â”€ planner.py                   # Query planning
â”‚   â”œâ”€â”€ executor.py                  # Tool execution
â”‚   â”œâ”€â”€ critic.py                    # Quality evaluation
â”‚   â”œâ”€â”€ query_analyzer.py            # Routing logic
â”‚   â”œâ”€â”€ memory_resolver.py           # Context resolution
â”‚   â”œâ”€â”€ entity_extractor.py          # Entity extraction
â”‚   â””â”€â”€ tools/                       # 11 specialized tools
â”‚       â”œâ”€â”€ vector_search.py
â”‚       â”œâ”€â”€ company_extractor.py
â”‚       â”œâ”€â”€ comparison.py
â”‚       â”œâ”€â”€ eligibility.py
â”‚       â”œâ”€â”€ answer_generator.py
â”‚       â”œâ”€â”€ trend_analyzer.py
â”‚       â”œâ”€â”€ branch_stats.py
â”‚       â”œâ”€â”€ skill_demand.py
â”‚       â”œâ”€â”€ company_cluster.py
â”‚       â”œâ”€â”€ recommendation_engine.py
â”‚       â””â”€â”€ sql_query.py
â”œâ”€â”€ streamlit_ui_v2.py               # Web interface
â”œâ”€â”€ query_helper.py                  # Basic RAG
â”œâ”€â”€ llm_reasoner.py                  # Answer refinement
â”œâ”€â”€ input_validator.py               # Security validation
â”œâ”€â”€ feedback_collector.py            # User feedback
â”œâ”€â”€ chunks_generation.py             # Data chunking
â”œâ”€â”€ embeddings.py                    # Embedding generation
â”œâ”€â”€ pinecone_upsert.py              # Vector DB upload
â”œâ”€â”€ deepseek_ocr.py                 # OCR processing
â”œâ”€â”€ requirements.txt                 # Dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸ”§ Data Processing Pipeline

### Offline Phase (One-time setup)

1. **Document Collection**: Gather PDFs, DOCX, PPTX files
2. **OCR Processing**: Extract text from scanned documents
3. **Chunking**: Split into 512-token chunks with overlap
4. **Classification**: Label sections (Eligibility, Compensation, etc.)
5. **Embedding**: Generate 384D vectors
6. **Upload**: Push to Pinecone vector database

### Real-time Phase (Per query)

1. **Validation**: Security checks, sanitization
2. **Routing**: Determine Basic vs Agentic mode
3. **Processing**: Execute search/tools
4. **Quality Check**: Critic evaluation
5. **Response**: Format and display with sources

## ğŸ¯ Query Types Supported

| Category | Examples |
|----------|----------|
| **Single Company Info** | "What is Intel's CTC?" |
| **Multi-Company Comparison** | "Compare Amazon vs Google" |
| **Statistical Analysis** | "Average CTC of all companies" |
| **Eligibility Check** | "Am I eligible with 8.5 CGPA?" |
| **Interview Prep** | "Google interview questions" |
| **Timeline Info** | "When is Amazon visiting?" |
| **Trend Analysis** | "CTC trends over years" |
| **Branch Analysis** | "CSE vs ECE placements" |
| **Skill Analysis** | "Most demanded skills" |
| **Personalized Recommendations** | "Best companies for me" |

## ğŸ”’ Security Features

- Input length validation (3-2000 characters)
- SQL injection prevention
- XSS attack filtering
- Rate limiting for abuse prevention
- Sanitized user inputs

## ğŸ“Š Performance Metrics

- **Simple queries**: ~2 seconds response time, 95% accuracy
- **Complex queries**: ~4 seconds response time, 82% accuracy
- **Data coverage**: 50+ companies, 3 years (2023-2025)
- **Vector database**: ~5000 chunks, 384D embeddings

## ğŸ¤ Contributing

This is an academic project developed for MSIS, MAHE. Contributions are welcome!

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author

**Manjus2003**
- GitHub: [@Manjus2003](https://github.com/Manjus2003)
- Institution: MSIS, MAHE

## ğŸ™ Acknowledgments

- MSIS Placement Cell for data access
- HuggingFace for model hosting
- Pinecone for vector database
- Streamlit for UI framework

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ for MSIS, MAHE**
